{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN ( comvolution neural network ) --> 망을 깊게하는 그런 과정을 파악하는 것이 중요하다 ( 깊게 하기 위해 머리를 쓰는 것이 중요)\n",
    "\n",
    "# -> pixel 의 주변을 적분한다 \n",
    "# -> filter, padding, stride, con2D 이용\n",
    "##  ( 필터의 사이즈가 중요하다 -> 필터사이즈가 크면 큰 특징을, 작게하면 작은 특징을 뽑아낸다 ) \n",
    "# -> pooling 이용 \n",
    "\n",
    "#VGG ( 망을 깊게 함 ) \n",
    "\n",
    "#Inception : 망을 깊게하기전에 수평으로 늘어뜨렸다.  ( inception 늘여뜨려보니 정확해져간다. )  \n",
    "\n",
    "# image generator : 이미지 증강을 한다. ( 이미지 부족 문제를 해결해준다 ) <- 전이학습도 데이터 부족문제를 해결한다 ( transduction ) \n",
    "\n",
    "\n",
    "# static mode ( 모델 다 만들고나서 마지막에 실행시켜봐야 어떤 데이터가 있는지 그떄서야 확인 가능하다 ) \n",
    "# dynamic mode ( = graph mode, static mode 와 다르게 데이터확인할 수 있으나 속도는 static mode 를 따라잡진 못한다 ) \n",
    "\n",
    "# efficientNet(필터사이즈, 깊이) , ResLet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform learning 대세다 ( 범용적으로 학습을 한다 )\n",
    "\n",
    "# global average pooling  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,Flatten  # flatten -> CNN 결과를 1차원으로 만들어준다. \n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first') # channels_first 로 할지, last 로 할지 정해주는 것이다!! \n",
    "\n",
    "batch_size= 128\n",
    "num_classes = 10\n",
    "epochs=12\n",
    "img_rows, img_cols= 28,28\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# channel_first, channel_last : 이미지 들어오는 방법 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 ( 차수 조정, categorical, 정규화등  ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)  ## >>>channel_first이다 ( channel 즉 1이 앞에 있으니까) \n",
    "# x_test = x_test.reshape(x_test.shape[0], 1, 28,28 )   \n",
    "#print(x_train.shape)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28,1 )  ## >>>channel_last이다 ( channel 즉 1이 뒤에 있으니까) \n",
    "x_test = x_test.reshape(x_test.shape[0],28,28, 1 ) \n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')  # 타입을 바꾸는 이유는? >> 255로 나누기 위해서 \n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train.shape:', x_train.shape) \n",
    "print(x_train.shape[0],'train samples') \n",
    "print(x_test.shape[0], 'test samples')  \n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)  # y_Train 과 y_test 를 categorical 로 바꿔줘야한다!!!! \n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', input_shape= (28,28,1))) #input shape 도 channel last 로 해줘야한다.(위에서 그렇게했으니까)\n",
    "          # >> 32 는 '필터개수' , 필터사이즈는 3x3 이란 뜻이다.\n",
    "          # >> 28x28 이 input 으로 들어왔고,  output 되는 출력값은 (128,26,26,32) 이다. (padding=valid 일때 이다)\n",
    "          #                                   output 이 (128,28,28,32) 이면  padding=same 일때 이다.(모서리특징유지됨)\n",
    "        \n",
    "        \n",
    "model.add(Conv2D(64 ,(3 , 3), activation='relu')) \n",
    "         # output 은 128 , 24 , 24 , 64 가 된다. (128장이 들어왔고 사이즈는 24x24)\n",
    "    \n",
    "    \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))  # output 은 128,12,12,64\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())  # Flatten -> 1차원 배열로 바꿔준다 --> 1장에대해서 12x12x64= 9216으로 바뀐다 -> flatten 시켰으니 이제부터는 FFNN 망이다.\n",
    "model.add(Dense(128, activation='relu'))  # 128(들어오는장) x 128(열값이라고생각하자)은 출력차수 즉,Dense 망의 가중치는 ?? 9216x128 이다. \n",
    "\n",
    "model.add(Dropout(0.5))  # 계산회로만 생략\n",
    "model.add(Dense(10, activation='softmax')) #>> 128 x 10 으로 출력이 된다. \n",
    "\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  모델 compile ,fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# epoch 마다 accuracy 를 저장하게 해준다.  ( 안해도 별차니는 없다 ) \n",
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self,logs={}): # 오버라이팅 ( -> 반드시 이름이 on_train_begin 으로 ! )\n",
    "        self.acc=[]              #>> 훈련시작시 이벤트(on_train_begin) 가 발생하도록 발생했다 \n",
    "    def on_epochs_end(self, batch, logs={}):   # epoch 하나 끝날때마다 하도록 설정 \n",
    "        self.acc.append(logs.get('acc'))\n",
    "history = AccuracyHistory()  # 인스턴스 해주고 아래 fit에 , callbacks에  history 지정한다.\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size,epochs=epochs, verbose=1, validation_data=(x_test,y_test), callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test,y_test,verbose=0)\n",
    "# print(score[0])\n",
    "\n",
    "# print(score[1])  -> accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS(hadoop file system) : model 구조 전체를 저장\n",
    "# 가중치, 구조, optimization, stage 등이 저장\n",
    "# json 으로 저장할때는 구조, 가중치를 별도로 저장해야한다. => json 쓰는이유는? web에서도 tensorflow 를 쓰기때문이다. ( tensorflow.js import)\n",
    "model.save('model_mnist.h5')\n",
    "print('모델이 저장되었습니다')\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model_mnist.h5')\n",
    "print('모델이 로딩되었습니다')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이어 정보 확인\n",
    "l1 = model.layers[0]\n",
    "l2 = model.layers[1]\n",
    "\n",
    "print(l1.name)\n",
    "print(l2.name)\n",
    "print(l1.input_shape)\n",
    "print(l1.activation)\n",
    "print(l1.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "from keras import backend as K\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28,28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28,28,1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=90)\n",
    "#datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
    "#datagen = ImageDataGenerator(width_shift_range=0.2, height_shift_range=0.2)\n",
    "#datagen = ImageDataGenerator(zca_whitening =True)           #>>>>>>>>>>     이미지 백색화 시키는법 -> noise 제거 (변수간 상관도를 없앤다 ( 즉 이미지의 pca 라고 생각하자 ))\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9,\n",
    "    save_to_dir = 'images', save_prefix='aug', save_format='png'):   # 코드의미 : images 폴더에 저장됐는데 증강이 됐다 -> augment = 증강됐다. \n",
    "    for i in range(0,9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28,28), cmap=pyplot.get_cmap('gray'))\n",
    "    pyplot.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar10 CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로딩 후 train, test 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10       # cifar10 은 10개로 구성 (airplane, automobile, bird,cat,deer, dog,frog,horse,ship,truc 분류문제)\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Dropout,Flatten, Activation\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "tf.reset_default_graph()\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "print('X_train shape : {}'.format(X_train.shape))\n",
    "print('X_train shape : {}'.format(X_train.shape[0]))\n",
    "print('X_test shape : {}'.format(X_test.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y값 범주화(categorical), 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CALSSES = 10\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CALSSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CALSSES)\n",
    "\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 확인\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[5])\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "plt.imshow(X_train[6])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 만들기(망깊게만들기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IMG_CHANNELS=3\n",
    "IMG_ROWS=32\n",
    "IMG_COLS = 32\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20  # 40으로 바꾸자 \n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "model = Sequential()\n",
    "# residual 망의 영향  (conv를 2번 하고 나서 pooling 을 해주면 residual 의 영향을 받아서 더 좋다 ) -> 망을 더 깊게하기 때문이다\n",
    "\n",
    " #input=32x32x3(=칼라) -> 가중치 = 32x32x32 \n",
    "model.add(Conv2D(32,kernel_size = 3, padding='same', input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, kernel_size=3, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64,kernel_size = 3, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64,3,3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs= NB_EPOCH, validation_split=VALIDATION_SPLIT, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(      \n",
    "    featurewise_center=False,\n",
    "    samplewise_center = False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,         #>>>>>>>> 이미지 백색화는 이미지 PCA ( 이미지 노이즈 제거 효과 있다 ) \n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip = True,\n",
    "    vertical_flip=False)\n",
    "datagen.fit(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator 하고 나서 model 에 다시 fit 시킬때는 아래처럼 한다. \n",
    "#= 케라스에서는 모델을 학습시킬 때 주로 fit() 함수를 사용하지만\n",
    "# 제네레이터로 생성된 배치로 학습시킬 경우에는 \n",
    "# fit_generator() 함수를 사용합니다.\n",
    "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "                   samples_per_epoch=X_train.shape[0], nb_epoch=NB_EPOCH, verbose=VERBOSE)\n",
    "\n",
    "# flow : 흘러들어가진다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, batch_size = BATCH_SIZE,verbose=VERBOSE )\n",
    "print('\\nTest Score:', score[0])\n",
    "print('\\nTest accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 파일 형식으로 저장하기 \n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "\n",
    "# weights 를 h5 파일 포맷으로 만들어 저장하기 ( json 은 가중치 따로 저장)\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 저장된 json 파일로부터 모델 로드하기\n",
    "from keras.models import model_from_json\n",
    "json_file = open('cifar10_architecture.json', 'r')\n",
    "loaded_model_json=json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# 로드한 모델에 weight 로드하기 \n",
    "loaded_model.load_weights('cifar10_weights.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q . 1. accuracy graph 를 출력해보시오 \n",
    "#    2. 이미지 cat, dog 를 다운로드한 후 다음 위의 모델로 예측해보시오 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   \n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names = ['cat.jpg', 'dog.jpg']\n",
    "imgs = [resize(imread(img_name), (32,32)).astype('float32') \n",
    "        for img_name in img_names]\n",
    "imgs = np.array(imgs) / 255\n",
    "predictions = model.predict_classes(imgs)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전이학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전이(transfer learning) 학습 application\n",
    "# 반지도학습 : 일부데이터에만 라벨이 없다. ( Knn : 주변에 있는 비슷한 것의 라벨을 가져온다 ) , Trenductive SVM \n",
    "#- 가중치 활용 : 그대로 적용\n",
    "#- 가중치 중 일부만 활용\n",
    "#- FFNN 부분만 학습해서 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained, fine-tuning  ( FFNN 부분만 domin knwledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESNET : ( 입력되는 데이터를 identity 시켜줘서 더해주는 망을 만든다 ) ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "import numpy as np\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications import resnet50\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 전처리\n",
    "filename = 'banana.jpg'\n",
    "original = load_img(filename, target_size=(224,224))  #어떤이미지가 들어오더라도 사이즈를 (224,224) 로 고정해준다. \n",
    "print('PIL image size', original.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(original)\n",
    "plt.show()\n",
    "numpy_image = img_to_array(original)  # 이미지를 배열로 만들어준다. \n",
    "plt.imshow(np.uint8(numpy_image))\n",
    "print('numpy array size' , numpy_image.shape)   # 차원확대 : 여러장 처리, 1장( 3차원->4차원으로 확장)\n",
    "image_batch=np.expand_dims(numpy_image, axis=0)\n",
    "print('image array size' , image_batch.shape)   \n",
    "\n",
    "\n",
    "#prepare the image for the resnet50 model\n",
    "precessed_image = resnet50.preprocess_input(image_batch.copy())\n",
    "# dense 이용해서 모델을 생성, 라벨 -> 확률\n",
    "\n",
    "resnet_model = resnet50.ResNet50(weights = 'imagenet')  # resnet50 이 쓰는 가중치 imagenet \n",
    "predictions = resnet_model.predict(precessed_image)\n",
    "label = decode_predictions(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label)  # 예상되는 목록 5개가 나옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras\n",
    "# fine_tuning : FFNN 부분만 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet, vgg16() -> 동영상 처리에도 많이 쓴다. \n",
    "# vgg16 : 깊은망임 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import randn\n",
    "import pathlib\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib.image import imread\n",
    "from keras.preprocessing import image\n",
    "# tf.enable_eager_execution()  -> 2.0 이 아닌경우 맨처음에 실행해줘야한다\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/Users/ICT01_17/Downloads/전달/flowers/flower_photos'\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "label_names= {'daisy':0, 'dandelion':1, 'roses':2, 'sunflowers':3, 'tulips':4}\n",
    "label_key = ['daisy','dandelion','roses','sunflowers','tulips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = list(data_dir.glob(\"*/*\")) # 모든데이터의 파일명을 읽어온다. \n",
    "all_images = [str(path) for path in all_images]  \n",
    "random.shuffle(all_images) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # all_images 는 화일명인데 이름에 대한 라벨을 할당해줘야한다   # pathlib.Path 는 디렉터리의 경로를 관리하는 라이브러리다. (?)\n",
    "all_labels = [label_names[pathlib.Path(path).parent.name] for path in all_images]  # >>> 라벨을 달아주는 코드이다.\n",
    "\n",
    "data_size = len(all_images)\n",
    "train_test_split=(int)(data_size*0.2)  # 20: 80로 나눴다. \n",
    "x_train=all_images[train_test_split:] # 이미 위에서 shuffle 해줬으니 이렇게 앞을 불러오고 뒤를 불러오는 방식으로 해도 괜찮다. \n",
    "x_test=all_images[:train_test_split]\n",
    "y_train=all_labels[train_test_split:]\n",
    "y_test = all_labels[:train_test_split]\n",
    "IMG_SIZE=160\n",
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\autograph\\converters\\directives.py:119: The name tf.read_file is deprecated. Please use tf.io.read_file instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def _parse_data(x,y):\n",
    "    image = tf.read_file(x)  # 데이터 읽는다. \n",
    "    image = tf.image.decode_jpeg(image, channels= 3) # 데이터를 jpeg 파일로 해독해라(읽는다) ,채널은3개다=칼러이미지다. \n",
    "    image = tf.cast(image,tf.float32)\n",
    "    image = (image/127.5) -1      # 데이터를 나누기위해서 float 형으로 바꿈-> 왜 127.5로 나누고 -1인가? 이미지를 -1에서 1사이즈로 만들기위해서 \n",
    "                                  # >> 이미지 사이즈를 통일시켜준다. \n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE)) # 이미지 사이즈 통일해준다 ( 160x160)\n",
    "    return image,y \n",
    "\n",
    "def _input_fn(x,y): \n",
    "    # 기본단위 요소로 분해 \n",
    "    ds = tf.data.Dataset.from_tensor_slices((x,y))   #imageDataGenerator 와 비슷한 역할을 하는 것= from_tensor_slices 로 dataset 을 생성해준다. \n",
    "    ds = ds.map(_parse_data)  # map? 언제쓰나 ? 함수를 적용할때 사용한다. ( 데이터마다 함수를 적용할때 사용) \n",
    "                            # parse_data 는 위에나온것\n",
    "    ds = ds.shuffle(buffer_size=data_size)  #shuffle 하려면  충분한 공간확보를 해야하므로 data_size 를 해줬다. \n",
    "    # shuffle : 뒤섞어라 \n",
    "    ds = ds.repeat()  # 데이터가 부족하면 처음부터 다시 시작하라는 뜻 \n",
    "    ds = ds.batch(BATCH_SIZE)  # batch 는 데이터를 한번에 읽어들일 minibatch size 를 말한다. \n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)  # 데이터를 읽어올때 시간이 손실이되니까 너가 알아서 조정해라 (autotune) : 몇개의 데이터를 읽어올지 너가 정해라\n",
    "     # 즉 학습할때는 batchsize 만큼 주지만 몇개를 읽을지는 너가 알아서 정하라는뜻! \n",
    "    return ds\n",
    "\n",
    "\n",
    "train_ds = _input_fn(x_train,y_train)\n",
    "validation_ds = _input_fn(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ICT01_17\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "VGG16_MODEL = tf.keras.applications.VGG16(input_shape=IMG_SHAPE, include_top=False, weights='imagenet') \n",
    "# 이미 tf안에 keras 안에 applications 안에 vgg16 모델 만들어져있다. \n",
    "# include_top=False 인경우에는 반드시 input_shape=IMG_shape 을 지정해야한다. \n",
    "# wieghts 는 이미 학습된 것을 사용한다. ( imagenet) \n",
    "#>> 즉vgg16 에는 convolution 에 대한 가중치 학습이 없다. ( 학습시켜둔것 그대로 쓰고 있다 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_MODEL.trainable =False  # >>>>> vgg16 모델에는 flatten 이 없다. ( 그걸 대신 하는 것이 아래 globalaveragepooling2d 이다.) # 원래, flatten 할때 가중치가 필요)\n",
    "\n",
    "\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "\n",
    "prediction_layer = tf.keras.layers.Dense(len(label_names),activation='softmax') # ffnn 의 가중치는 학습이 됐다. ->이렇게만 했는데 80퍼가 나옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential([ VGG16_MODEL, global_average_layer, prediction_layer]) \n",
    "                         # -> 이렇게 묶어준다. ( 즉 model 은 vgg16가중치를와 FFNN 의 가중치학습을 이용해서 80퍼를 만들어냄 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),  # tf안에 있는 adam 을 지정해줬다. 근데 loss 는 keras 안에 있는 것을 사용했다. \n",
    "             loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "# sparse_categorical_crossentropy 은 희소행렬을 의미한다. --> 분류하는 것이 많을때 쓴다. (한 1000개 정도로 분류될때 쓰기 좋다 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2 steps\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 11s 5s/step - loss: 1.9631 - acc: 0.1875\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 2.1081 - acc: 0.1094\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.8612 - acc: 0.1250\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.6333 - acc: 0.2812\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 1.7663 - acc: 0.1562\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.6640 - acc: 0.2344\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.6170 - acc: 0.2500\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 181ms/step - loss: 1.6424 - acc: 0.1250\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.6274 - acc: 0.2812\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.6211 - acc: 0.4219\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.6074 - acc: 0.3594\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 1.5178 - acc: 0.4219\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.5566 - acc: 0.3750\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 193ms/step - loss: 1.5668 - acc: 0.3281\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.4915 - acc: 0.3594\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 1.5790 - acc: 0.2969\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 193ms/step - loss: 1.5137 - acc: 0.2969\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.4546 - acc: 0.4844\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 1.5333 - acc: 0.3906\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 1.4260 - acc: 0.4531\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 193ms/step - loss: 1.3703 - acc: 0.5312\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.4950 - acc: 0.4688\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 1.4434 - acc: 0.4219\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 1.5131 - acc: 0.4219\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 1s 258ms/step - loss: 1.3773 - acc: 0.4844\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 220ms/step - loss: 1.4147 - acc: 0.5156\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 1.3656 - acc: 0.5469\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 1.3848 - acc: 0.5000\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.3443 - acc: 0.5469\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 1.3282 - acc: 0.5781\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.3325 - acc: 0.5469\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 1.3745 - acc: 0.5156\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 186ms/step - loss: 1.3324 - acc: 0.5312\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.3546 - acc: 0.5000\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 193ms/step - loss: 1.3063 - acc: 0.5312\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 1.2826 - acc: 0.6562\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.2671 - acc: 0.6250\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.3014 - acc: 0.5312\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 181ms/step - loss: 1.2771 - acc: 0.4844\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.2462 - acc: 0.6094\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.2216 - acc: 0.5781\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 186ms/step - loss: 1.3644 - acc: 0.4531\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 183ms/step - loss: 1.2582 - acc: 0.4844\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 1.2677 - acc: 0.6094\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 181ms/step - loss: 1.2159 - acc: 0.6562\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 194ms/step - loss: 1.2188 - acc: 0.5938\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 1.1232 - acc: 0.6562\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.2775 - acc: 0.5625\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.0816 - acc: 0.7344\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.1911 - acc: 0.5781\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.2718 - acc: 0.5469\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 1.1871 - acc: 0.6562\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 1.2417 - acc: 0.5469\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 180ms/step - loss: 1.1200 - acc: 0.6406\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 1.1717 - acc: 0.6094\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 1.1727 - acc: 0.6250\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 1.1244 - acc: 0.6406\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 1.1549 - acc: 0.6094\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.1079 - acc: 0.6094\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 183ms/step - loss: 1.1309 - acc: 0.5938\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.1712 - acc: 0.6250\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 193ms/step - loss: 1.1598 - acc: 0.5938\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 1.0796 - acc: 0.6875\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 187ms/step - loss: 1.1133 - acc: 0.7188\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 180ms/step - loss: 1.0856 - acc: 0.6406\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.1013 - acc: 0.7031\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 1.0660 - acc: 0.7031\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 194ms/step - loss: 1.1218 - acc: 0.5938\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.0257 - acc: 0.7031\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.1366 - acc: 0.5625\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 1.1087 - acc: 0.6562\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 177ms/step - loss: 1.1466 - acc: 0.5625\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 1s 269ms/step - loss: 1.1639 - acc: 0.5938\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 186ms/step - loss: 1.0636 - acc: 0.7031\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 1.1021 - acc: 0.6562\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 1.0311 - acc: 0.6562\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 181ms/step - loss: 0.9957 - acc: 0.6875\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 193ms/step - loss: 1.0654 - acc: 0.7188\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 1.0330 - acc: 0.7344\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.0986 - acc: 0.5781\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 1.0257 - acc: 0.7344\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 0.9392 - acc: 0.7969\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 0.9354 - acc: 0.8281\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 0.9947 - acc: 0.6875\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 0.9982 - acc: 0.7188\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 1.0202 - acc: 0.6719\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 180ms/step - loss: 0.9689 - acc: 0.7188\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 193ms/step - loss: 0.9367 - acc: 0.7031\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 1.0797 - acc: 0.6406\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 1.0617 - acc: 0.6875\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.0829 - acc: 0.6250\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 0.9810 - acc: 0.6562\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 1.0003 - acc: 0.6875\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 181ms/step - loss: 0.9335 - acc: 0.7188\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 0.9760 - acc: 0.7656\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 0.9925 - acc: 0.7188\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 0.9556 - acc: 0.7188\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 0.9007 - acc: 0.7500\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 0.9097 - acc: 0.7344\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 0.8522 - acc: 0.8281\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=100, steps_per_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1c6ea0624647>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_acc'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
