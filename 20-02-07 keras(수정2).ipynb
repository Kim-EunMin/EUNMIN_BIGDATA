{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝(ML) : 입력을 결합하여 이전에 본 적이 없는 데이터를 적절히 예측/ 분류하는 방법을 학습\n",
    "\n",
    "# 구분1 : 지도학습( 입력, 정답이 있다 ) \n",
    "# 구분2 : 비지도학습( 입력만 있다 ) ex. 자연어처리에서 유사한 단어끼리 묶기, 이미지 비슷한 것끼리 묶기 \n",
    "# 구분3 : 강화학습 ( 상호 피드백을 통해서 최적의 동작을 학습 )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 구성하기 전에 예측인지 분류인지 고려해야 한다\n",
    "\n",
    "# 선형회귀 -> 예측\n",
    "# 로지스틱 회귀 -> 분류\n",
    "# 소프트맥스 회귀- > (다중클래스를) 분류  ( y값이 여러개다 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 딥러닝 라이브러리\n",
    "#2. 사용하기 쉬운 고차원 딥러닝 API\n",
    "#3. Tensorflow 기반 API를 이용한 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. keras 의 backend : tensorflow, CNTK, Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 생성 (numpy, pandas 이용) \n",
    "\n",
    "# 모델 구성하기 : model = Sequential() +  model.add()->여기서 dense 추가 ( dense = 계층 !)\n",
    "# ex. model.add(Dense(1, input_dim=1, activation='linear'))  : 1은 출력인자를 말한다 , input_dim : 입력인자(x값 개수) , activation\n",
    "\n",
    "# 학습과정 설정하기 ( model.compile() 을 통해서 손실함수, 최적화 등을 하는 곳) \n",
    "\n",
    "# 모델학습하기 ( model.fit() )\n",
    "# --> ex. model.compile(optimizer=sgd, loss='mse', metrics=['accuracy'], shuffle =False)  # shuffle : 데이터셋 랜덤하게 섞는다 \n",
    "\n",
    "# 모델 평가 ( model.evaluate() )  \n",
    "\n",
    "# 모델예측 ( model.predict() )\n",
    "\n",
    "\n",
    "\n",
    "# ex. lr = 0.01 , batch_size=1, epochs=50,등등  -> 이런 건 우리가 정의해서 넣는 값들로 hyperparameter 이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shuffle 써보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수 : activation  => linear, sigmoid, softmax, relu 등 \n",
    "# 손실함수 : loss ( 실제값-예측값)   =>  mse, binary_crossentropy(이진분류일때 사용, 시그모이드랑 많이 쓰임)\n",
    "#            categorical_crossentropy (3개이상분류일때 자주쓰이고, softmax 랑 많이 쓰인다 ) \n",
    "# 최적화 방법 : optimizer (기울기 갱신하면서 손실 최소화 )  \n",
    "#             =>   GD( 경사하강법 ) , SGD( 확률적 경사하강법 ) , Momentum\n",
    "# 그 외\n",
    "#   =>  lr(학습률), batch, batch_size, epochs , overfitting(과적합), one-hot-encoding, 정규화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras ( 선생님 필기 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras 이전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지금까지 \n",
    "# constant , variables( 가중치 저장되는 공간 ) , placeholder( 데이터 주입) 를 사용해서 \n",
    "# 가중치와 들어오는 데이터와 행렬연산을 통해 오차기반 학습을 했다.\n",
    "\n",
    "# for 문을 이용해서 epoch 와 batchsize 를 돌렸다  \n",
    "# ---> keras 에서는 for문 안쓰고, fit 함수를 이용해서 한번에 가능하다 (tensorflow 는 그 fit 내용을 이해하기 위해 배웠음)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras 의 backend :  tensorflow, CNTK, Theano ( -> 이런것등을 추가해서 묶은것이다 =>  keras 는 3가지등을 wrapper 시킨것이다. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras의 compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras 의 compile  (compile 은 loss, activation, optimizer 이런것을 말함. compile 아래에 저런것들이 있음 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://www.codeonweb.com/entry/eda18bec-7c7d-426f-ab98-90e18db6fdba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras 는 model 을 layer 방식으로 만들었다( = 가중치 선언 할 필요가 없다), keras에서는 return 받는 과정이 생략된다 ( 이젠 점찍고 함수 호출함 )\n",
    "# layer 로 만들어졌기때문에 다른것들도 layer 로 구성되어있는데 Input, Dense, CNN, RNN 모두 layer 로 만들었다 \n",
    "# 즉 keras 는 출력 차수 값만 지정하면 된다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras 에서 model 을 만드는 방법\n",
    "# 1. sequential  ( 간단한 망, simple input, simple output) \n",
    "# 2. functional ( multi input, multi output ( ex. 한쪽은 이미지 한쪽은 텍스트를 받아서 RNN 망 학습시키면 이미지를 텍스트로 번역할수있다))\n",
    "# 3. model  ( 상속을 받아서 하는 것으로 , class base 로 프로그램을 하게 만들어줬다 )\n",
    "\n",
    "\n",
    "# evaluate 하고 predict  ( evaluate 에는 test 데이터 넣어주고 predict 에는 실제 사용할 데이터 넣어주면 된다 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sequential 이 model 과 다른 것은, 모델의 맨 앞에 input 이 없다는 것이다.\n",
    "# sequential 의 경우, 첫번째 레이어에서 input_shape 에 input 데이터 형태를 함께 넘겨준다 \n",
    "seq_model = Sequential([Dense(512, input_shape=(784,),activation='relu')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras 의 hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  keras 는 scikit 과 연결되어있다.\n",
    "# scikit 의 장점은 pipeline, gridsearchcv  \n",
    "# 사이킷에 있는 gridsearchcv 를 이용해서 최적의 하이퍼파라미터를 정해질 수 있게끔 해준다 (즉 keras 는 하이퍼파라미터 튜닝 기능 사용할 수 있다 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application transform learning(처음부터 가중치를 학습시키는 것은 오래걸림)\n",
    "# 즉 대표적인 분야에서 pretraining 을 해둔다 ( pretrainging 하면 어느정도 가중치가 학습되어있다 ) \n",
    "# 가중치를 처음부터 학습하지 않고 -> 그니까 빨리 적합할수있다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그 외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU  ( LSTM 보다 속도 빠름 ) \n",
    "# GRU 에서 sequence2sequence 했음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMT -> transformer -> BERT (transform 이용해서 양방향으로 작동하게 하는 망) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image , movie -> opencv\n",
    "# sound  -> librosa\n",
    "# text -> mltk, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코드 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오늘 : 파라미터 튜닝, 기본구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 회귀\n",
    "from keras.models import Sequential  # Sequential -> models 에 있는 서브패키지\n",
    "from keras.layers import Dense       # layers 에서 모델 구성을 한다 ( Dense 등 ) \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler  #  정규화하기 위해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 데이터 준비 :  make_regression :  회귀분석용 가상데이터를 만들어준다 \n",
    "X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=1)  \n",
    "#print(X.shape)    # 샘플100개, feature은 2니까 -> X 는 100x2\n",
    "#print(y.shape)    # y 는 100개가 만들어진다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.정규화\n",
    "scalarX, scalarY = MinMaxScaler(), MinMaxScaler()  # 민맥스 2개 만든 이유는 ? 데이터 사이즈가 다르기 때문이다\n",
    "scalarX.fit(X)\n",
    "scalarY.fit(y.reshape(100,1))  # 행으로 만든다\n",
    "y = scalarY.transform(y.reshape(100,1))\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. keras 모델만들기\n",
    "model = Sequential() # sequential 모델 사용했다\n",
    "\n",
    "model.add(Dense(4, input_dim=2, activation='relu'))  # dense: 계층/  dense는 출력차수만 지정하면 가중치를 알아서 만들어준다 \n",
    "                                                    # 4 는 출력인자, input_dim 은 입력인자 ( x 그니까 열의 갯수라고 생각하자)\n",
    "                                                     # --> 100x2 가 들어왔고, dense 가 가중치 2x4 를 만들어준다. -> 출력 100x4 \n",
    "\n",
    "model.add(Dense(4,activation='relu'))    # 두번째 층 입력망은 100x4 이고 가중치는 4x4 이다. 따라서 출력차수는 100 x 4 가 된다. \n",
    "model.add(Dense(1, activation='linear'))  # 입력은 100x4 출력은 100x1 ( dense 가 알아서 정한 가중치는 4x1 )\n",
    "# >>> 즉 100x1 로 예측이 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 모델을 만들어두고 compile 을 하는 것은 ? backend 가 tensorflow 이기 때문이다 ( 즉 tensorflow 로 바꿔주는 것이 compile 이다 )\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')   # compile 은 tensorflow 로 모델을 변형하는 역할을 한다. \n",
    "###>> compile 쓰니까 간단하게 loss, optimizer 동시 사용이 가능 \n",
    "\n",
    "# loss 함수 :  회귀일땐 mse, 분류일때는 enthropy, gan 에서는 KL-divergence( 이건 분포를 비교해준다 ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 모델 적용\n",
    "model.fit(X,y, epochs= 1000, verbose=0)  # tensorflow 에서는 이중 for 문을 썼었다 ( 위에 for문은 epoch, 밑에 for문은 minibatch )\n",
    "\n",
    "# X : 100x2  ,  출력차수는 2x4 ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 테스트 해보려고 새로운 샘플을 만들었다.\n",
    "Xnew, a = make_regression(n_samples=3, n_features=2, noise=0.1, random_state=1)\n",
    "Xnew = scalarX.transform(Xnew)\n",
    "ynew = model.predict(Xnew)\n",
    "\n",
    "for i in range(len(Xnew)):\n",
    "    print('입력데이터={}, 예측결과={}'.format(Xnew[i], ynew[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(Xnew)):\n",
    "#    print('입력데이터=%s, 예측결과=%s' % (Xnew[i], ynew[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras에서는 model 에 add 해서 layer 를 추가하면 된다 -> keras 가 알아서 layer 를 많이 만들어뒀다. 그 중에 많이 쓰는 것이 dense 망이다\n",
    "# (dense 망은 FFNN 망을 만들때 쓴다) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pima.data 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential  # sequential 은 입력도 1개, output 도 1개\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "dataset = numpy.loadtxt('pima.data', delimiter=',' ) \n",
    "\n",
    "X = dataset[:,0:8] # 독립변수 8개\n",
    "Y = dataset[:,8]  # 9번째 있는 것이 종속변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 \n",
    "model = Sequential()  \n",
    "\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))  # ? x 8 ,  8 x 12  => ? x 12\n",
    "model.add(Dense(8, activation='relu'))                # ? x 12,  12 x 8   => ? x 8\n",
    "model.add(Dense(1, activation='sigmoid'))             # ? x 8 ,  8 x 1    => ? x 1 \n",
    "\n",
    "# 앞에 3.1 회귀에서는 마지막 layer의 활성화함수에 linear 가 왔다 ( 예측이 된다) \n",
    "# -> 여기선 sigmoid  ( 0.5보다 크면 1 아니면 0 이된다 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile -> 주로 loss, optimizer, metrics 를 쓴다. \n",
    "model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy']) \n",
    "model.fit(X, Y, epochs=150, batch_size=10 )  # epochs 와 batch_size 는 fit 에 들어온다\n",
    "\n",
    "scores = model.evaluate(X,Y)\n",
    "# print(model.metrics_names)       #>> 2번이 accuracy\n",
    "print('\\n%s: %.2f%%' %(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kernel_initalizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "numpy.random.seed(7)\n",
    "\n",
    "dataset = numpy.loadtxt('pima.data', delimiter=',')\n",
    "X = dataset[:,0:8] \n",
    "Y = dataset[:,8] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://keras.io/ko/initializers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_initializer?\n",
    "# >> 케라스 레이어의 초기 난수 가중치를 설정하는 방식을 규정한다. \n",
    "# >> 주로 kernel_initializer 또는 bias_initializer 를 사용 \n",
    "# TruncatedNormal 는 신경망에 많이 쓴다 (블로그 참고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu', kernel_initializer='uniform'))\n",
    "#kernel_initializer 를 통해서 초기화 방법 지정해줄 수 있다. -=> 우리도 모르게 만들어지는 가중치를 균등분포로 초기화해주었다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### return 을 사용했을때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X,Y, epochs=150, validation_split=0.33, batch_size=10)  # history 로 리턴받았다. -> 아래 참고\n",
    "\n",
    "# 데이터를 분리할때, train 와 test 로 분리되는데 과적합 되는 현상이 벌어진다. \n",
    "# --> 그래서 train, validation, test 로 나눈다 ( 검증용 데이터 = validation ) \n",
    "\n",
    "\n",
    "scores = model.evaluate(X,Y)\n",
    "print('\\n%s: %.2f%%' %(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys()) # history 로 리턴받으면 -->> loss, accuracy 가 둘다 나온다. -> val_loss, val_accuracy , loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_moons 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X , Y =make_moons(noise=0.2, random_state=0 , n_samples=1000)  # X 는 1000 x 2 \n",
    "\n",
    "X=scale(X)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[Y==0,0], X[Y==0,1], label='Class 0')\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], color='r', label='Class 1')\n",
    "ax.legend()\n",
    "ax.set(xlabel='X', ylabel='Y', title='binary classification ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[Y==0,0], X[Y==0,1], label='Class 0 ')\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], color='r', label='Class 1')\n",
    "ax.legend()\n",
    "ax.set(xlabel='X', ylabel='Y', title='binary classification ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비선형모델일때 -> 신경망쓴다\n",
    "import keras\n",
    "\n",
    "model = Sequential()  \n",
    "\n",
    "model.add(Dense(32, input_dim=2, activation='relu')) # 1000 x2   2x32  --> 1000x32 가 출력된다.  ( )\n",
    "# 행렬연산을 하면 설명이 확대된다 \n",
    "# 가중치  2x32= 64 + 32(바이어스) = 96   해서  --> 출력은 1000x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))  # 여기에는 1000x32 가 들어오고 가중치는 32x1 이 되고 출력결과는 1000x1 이 된다 ( 분류라는 것) \n",
    "                                        # 32x1 = 33 + bias(1) = 34  (즉 가중치가 있으면 bias 도 있다 )\n",
    "model.compile(optimizer='AdaDelta',  loss='binary_crossentropy', metrics=['accuracy']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tykimos.github.io/2017/07/09/Training_Monitoring/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback 함수는 언제쓰나?? >>  callback 함수는 window 에서 자동으로 호출되는 함수이다 (함수가 함수를 호출하는 그런것이라고 생각하자)\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir='./Graph/model_1/', # graph 이미지 출력 \n",
    "                                         histogram_freq = 100, write_graph=True,\n",
    "                                          write_images =False) \n",
    "\n",
    "# log_dir 인자에 경로를 넣는다 ( 이 경로에 텐서보드와 정보를 주고 받을 수 있는 파일이 생성된다 ) \n",
    "\n",
    "tb_callback.set_model(model)   # 콜백을 모델에 등록해준다 (이미지를 간단하게 불러오는것을 콜백함수가 쓴것이라고 생각하자)\n",
    "# 콜백함수를 모델에 등록하면 --> 모델이 훈련되면서 자동으로 위에 3문장을 호출해준다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 등록을 했고 fit 할때 호출을 한다 \n",
    "hist = model.fit(X_train, Y_train, batch_size=32, epochs=200, verbose= 0, validation_data = (X_test, Y_test), callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test Loss: \" , score[0])\n",
    "print(\"Test accuracy:\", score[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.print_summary(model)  #print 하고 summary 하면 모델의 구성을 출력한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model)  # 이건 모델의 구성!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "model_to_dot(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, SVG\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터준비\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pylab as plt\n",
    "(X_train0, y_train0), (X_test0, y_test0) = mnist.load_data()\n",
    "print(X_train0.shape, X_train0.dtype)   # 60000,  28x28 이 나온다\n",
    "print(y_train0.shape, y_train0.dtype)   # 60000\n",
    "print(X_test0.shape, X_test0.dtype)     # 10000, 28x28\n",
    "print(y_test0.shape, y_test0.dtype)     # 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train0[0])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully-connected : flatten   <- reshape 사용\n",
    "X_train = X_train0.reshape(60000,784).astype('float32')/255.0  # 255로 나누는 이유 : 이미지 정규화 \n",
    "                                                             #이미지는 원래 0~255 사이의 값이므로 255 로 나누니 0~1로 바뀐다= 정규화\n",
    "                                                             # 나누려면 float 형으로 바꿔야한다.\n",
    "X_test = X_test0.reshape(10000,784).astype('float32')/255.0\n",
    "\n",
    "print(X_train.shape, X_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train0[:5]  # >> 이걸 확인해보면 one_hot_encoding 여부를 확인할 수 있다.\n",
    "              # 5,0,4,1,9 로 나온다. -> one_hot_encoding 되어있지 않다 -> 원핫인코딩해주어야한다. !! \n",
    "              #  만약 강아지고양이 이렇게 말고 3개가 넘어가면 softmax 해준다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫인코딩 : categorical 함수를 이용한다!!\n",
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(y_train0, 10)\n",
    "Y_test = np_utils.to_categorical(y_test0, 10)\n",
    "Y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 만들기\n",
    "import numpy as np\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "model= Sequential()  \n",
    "model.add(Dense(15, input_dim=784, activation='sigmoid'))    # 60000x784,  784x15  -> 60000x15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 compile\n",
    "\n",
    "model.compile(optimizer=SGD(lr=0.2), loss='mean_squared_error', metrics=['accuracy'])  #lr= learning rate,\n",
    "                                                                            #SGD = 확률적 경사하강법 = stockhastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q . 출력차수가 512 인 레이어를 추가하고 싶을땐??\n",
    "#model.add(Dense(512, input_dim=784, activation='sigmoid')) \n",
    "#model.add(Dense(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 레이어 구성확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers)  # 레이어 구성 확인할 수 있다. \n",
    "print()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = model.layers[0]\n",
    "l2 = model.layers[1]\n",
    "\n",
    "# 레이어 속성으로 정보확인\n",
    "l1.name\n",
    "l1.input_shape\n",
    "l1.output_shape\n",
    "l1.activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, Y_train, nb_epoch=30, batch_size=100,\n",
    "                validation_data = (X_test, Y_test), verbose=2) # validation data 를 저렇게 별도로 주어도된다 \n",
    "plt.plot(hist.history['loss']) # 작업했던 내용을 확인할수있다. \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy'],'b-', label='training')\n",
    "plt.plot(hist.history['val_accuracy'],'r:', label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(hist.model)\n",
    "print(hist.params)\n",
    "print(hist.history['accuracy'])\n",
    "print(hist.history['val_accuracy'][29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback class\n",
    "# callback 함수를 상속받으면 custom callback 함수를 제작할 수 있다\n",
    "# >> keras 가 가지고있는 call back 기능은 함수에 의해서 호출되는 객체이다 \n",
    "import keras\n",
    "class CustomHistory(keras.callbacks.Callback): # () 안에 보이는것처럼 상속을 받는다. \n",
    "    def init(self):   #__init 랑 다른가??? (이해안됨)  \n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "    def on_epoch_end(self, batch,logs={}):  # 오버라이딩하고있다 (배치와 로고가 매개변수로 들어온다) \n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        self.train_acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_hist = CustomHistory()\n",
    "custom_hist.init()  # init 를 호출하면 안되는데 어떻게 호출했나? \n",
    "hist = model.fit(X_train, Y_train, nb_epoch=30, batch_size=100, validation_data = (X_test,Y_test),\n",
    "                 callbacks=[custom_hist], verbose=2)\n",
    "                 \n",
    "plt.plot(hist.history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy'],'-b', label='training')\n",
    "plt.plot(hist.history['val_accuracy'], 'r:', label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(hist.model)\n",
    "print(hist.params)\n",
    "print(hist.history['accuracy'])\n",
    "print(hist.history['val_accuracy'][29])\n",
    "plt.show()\n",
    "plot(custom_hist.train_loss, 'y', label='train loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 : X_test 의 이미지 한장의 label 을 예측하라\n",
    "model.predict(X_test[:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_classes(X_test[:1,:],verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOSTON 집값데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "from keras.datasets import boston_housing\n",
    "(x_train,y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델만들기\n",
    "model = Sequential()\n",
    "num_features = 13\n",
    "\n",
    "# 회귀만들기\n",
    "model.add(Dense(1, input_dim = num_features, activation='linear'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])     # rmsprop : 가중치 제어하여 학습률제어\n",
    "                                                                    # mae : 회귀에서 많이 쓰인다\n",
    "model.fit(x_train, y_train, batch_size=1, epochs=10, verbose=1)    #--> 회귀가 간단하게 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아까는 accuracy 였는데 지금은 mae?? (metrics 에 mae 를 지정했기 때문이다)\n",
    "mse, mae=model.evaluate(x_test, y_test, verbose=False)\n",
    "rmse = np.sqrt(mse)\n",
    "mse, rmse, mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. x_test 앞의 3개를 예측해보시오\n",
    "pred =model.predict(x_test[:3,:])\n",
    "# print(pred)\n",
    "# print(pred.shape)\n",
    "pred = pred.reshape(3,)\n",
    "\n",
    "real = y_test[:3]\n",
    "real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상관계수를 확인하라\n",
    "import numpy as np\n",
    "np.corrcoef(pred, real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikits 와 keras 를 연결하는 객체를 만들었다\n",
    "# --> 그게 바로 kerasclassifier,  kerasregressor 이다\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier  # 그리드서치씨브이랑 케라스클래스파이어 2개 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = np.loadtxt('pima.csv', delimiter=',')  ==>  768x9 ==> 768x8은 독립변수,  768x1 은 종속변수 ( 변수 8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation = 'relu'))  # 768x8 ,  8x12 -> 768x12\n",
    "    model.add(Dense(1, activation = 'sigmoid'))  # 0~1 값이 된다\n",
    "    model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataset = np.loadtxt('pima.csv', delimiter=',')\n",
    "print(dataset.shape)\n",
    "\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "model=KerasClassifier(build_fn = create_model, verbose=0)   # verbose : 설명을 달 것인가 말 것인가 (0,1)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 튜닝을 위해서 아래처럼 조합을 만든다 \n",
    "batch_size = [10,20] #40,60,80,100 도 더 넣어본다\n",
    "epochs=[1,3]         # 10,50,100\n",
    "\n",
    "# 매개변수 입력 형태는 dictionary 이다!\n",
    "param_grid=dict(batch_size=batch_size,  epochs=epochs)\n",
    "\n",
    "grid = GridSearchCV(estimator = model, param_grid= param_grid, n_jobs=-1)   # n_jobs 의 디폴트값은 1 (이 값을 증가시키면 내부적으로 멀티 프로세스를 사용하여 그리드서치를 수행한다, 만약 cpu 코어의 수가 충분하다면 n_jobs를 늘릴 수록 속도가 증가한다.\n",
    "grid_result = grid.fit(X,Y)\n",
    "\n",
    "\n",
    "print('최적스코어: {} 사용한 파라미터조합 {}: '.format(grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 경의 수에 대해 구해준다.\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, params in zip(means,stds,params):\n",
    "    print('{}({}) with : {}'.format(mean, stdev,params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q. dense1 : activation 함수를 달아주고\n",
    "# ['softmax', 'softplus', 'softsign',' relu', 'tanh', 'sigmoid', 'hard_sigmoid' , 'linear'] 의 조합중 가장 좋은 activation 함수를 ㅓㅇ하라 \n",
    "\n",
    "# dense2 : 가중치 초기화(kernel_initializer) 매개변수를 넣고 이를\n",
    "#['uniform ' , 'lecun_uniform', 'normal', 'zero',' glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "# 로 테스트해서 가장 최적의 가중치 초기화 파라미터를 결정하시오\n",
    "\n",
    "# dropout rate 도!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 튜닝을 위해서 아래처럼 조합을 만든다 \n",
    "activation = ['softmax', 'softplus'] # 'softsign','relu', 'tanh', 'sigmoid', 'hard_sigmoid' , 'linear' 도 추가해보기\n",
    "init_mode = ['uniform' , 'lecun_uniform'] # 'normal', 'zero','glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'              \n",
    "dropout_rate=[0.0, 0.1] # 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 도 추가가능\n",
    "\n",
    "import numpy\n",
    "from keras.layers import Dropout\n",
    "def create_model(activation, init_mode, dropout_rate): # 파라미터 지정 필수\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))  # dropout -> 과적합 방지\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=init_mode))  # 여기서는 create_model 에 파라미터를 지정했따\n",
    "                                             # 파라미터에 안넣으면 함수안에서 init_mode 를 인식하지 못함!!!\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=7\n",
    "numpy.random.seed(seed)\n",
    "dataset = np.loadtxt('pima.csv', delimiter=',')\n",
    "print(dataset.shape)\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0) \n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매개 변수 입력 형태는 dictionary 이다 \n",
    "param_grid = dict(init_mode = init_mode, activation = activation, dropout_rate= dropout_rate)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1) # 참여하는 cpu 개수 \n",
    "grid_result = grid.fit(X,Y)\n",
    "\n",
    "#summariez result\n",
    "print('최적스코어: {} 사용한 파라미터조합{} : '.format(grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print('{}({}) with : {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정리예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적의 파라미터 조합을 확인해보았다. 이번엔 다른 걸 해보자\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "\n",
    "\n",
    "#순서1. early stopping -> 과적합 막으려고 쓰는 것이다 ( callback 함수안에 들어가있다 ) + 여러 파라미터 지정(밑으로)\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "stopper = EarlyStopping(monitor='val_accuracy', patience=3, verbose=1) # val_accuracy 를 모니터하다가 멈춘다 ( patience-> 3번참아라)\n",
    "                                                                        #-> callback 함수는 fit 안에 넣는다.\n",
    "\n",
    "#>> 최적의 파라미터 찾는다 ( 위에 참고) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순서4 ,최적의 파라미터를 찾은 후 ,만들어둔 모델에 집어넣는다. \n",
    "def create_model(activation='uniform', optimizer='relu',dropout_rate=0.1): # 파라미터 지정 필수\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation=optimizer, kernel_initializer=init_mode))\n",
    "    model.add(Dropout(dropout_rate))  \n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=init_mode))  \n",
    "                                             # 파라미터에 안넣으면 함수안에서 init_mode 를 인식하지 못함!!!\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순서5. 모델을 만든다 (kerasClassifier 이용)\n",
    "model = KerasClassifier(build_fn = create_model, epochs=100, batch_size= 64)\n",
    "\n",
    "#순서2 define the grid search parameters\n",
    "dropout_rate=[0.1,0.2,0.3] # ,0.4,0.5,0.6,0.7,0.8,0.9\n",
    "init_mode = ['uniform', 'lecun_uniform']# ,'normal','zero','glorot_normal','glorot_uniform','he_normal','he_uniform'\n",
    "optimizer = ['softmax','relu'] #, 'softplus', 'softsign','tanh','sigmoid','hard_sigmoid','linear'\n",
    "\n",
    "#순서3 파라미터를 dict 로 묶는다. -> gridsearchcv 진행\n",
    "param_grid = dict(init_mode = init_mode, optimizer = optimizer , dropout_rate= dropout_rate)\n",
    "fit_params = dict(callbacks=[stopper])  # callback 은 fit 함수에 넣는다. \n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1) # 참여하는 cpu 개수 \n",
    "grid_result = grid.fit(X,Y, **fit_params)  # ** 를 하는 이유? 변동매개변수이기 때문(10개도 들어올수도있기때문, callback 등등 몇개 들어올지 모르니까\n",
    "                                             # 변동매개변수(여러개가 전달될때 쓴다 ) list 는 * , 변동매개변수는 **\n",
    "\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기까지 했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번엔 회귀를 해보자\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv('housing.csv', delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():#맨처음에 hyperparameter tuing 할때 epoch 랑batch_size 를 위에 안줬다  근데 2번째할때는 줬다(이유: model 안의 dense 의 매개변수이기때문에 줘야한다!!!!)\n",
    "    model = Sequential() # 몇바이몇으로 들어오냐?? ?? 506x13  13x13 이다.  \n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu' )) # 아까 그리드서치로 hyper parameter뭐가좋은지찾와봤음 -> 모델마다 다르다 \n",
    "    # 506x13  13x1  506x1 \n",
    "    model.add(Dense(1,kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')  #def 를 쓸때, COMPILE 하는부분까지만 정의한다 \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "estimator = KerasRegressor(build_fn = baseline_model, nb_epoch=100, batch_size=5, verbose=0) # keras 매개변수의 첫변째 옵션에 위에서 만든 모델 넣어주면 된다 \n",
    "                                       # epoch 랑 batch_size 는 피팅할때 주는것이다 ! ( 컴파일일때 아니다!) \n",
    "    # 아직 모델학습을 한 것은 아니고 선언만했다 근데 아래에서 cross_val_score 을 한다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=seed) # kfold : 교차검증할때 사용 -> 10개로 나누고 10개중 하나는 validation 으로 사용된다 \n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)# 모델선언만했지만 cross_val_score 이 가능하다! \n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(seed)\n",
    "estimators= []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn = baseline_model, epochs=50, batch_size=5, verbose=0))) # kerasregressor 같은것은 딕셔너리를 매개변수로 요구 \n",
    "\n",
    "# pipeline 의 매개변수가 리스트를 요구한다!!(맨위를 봐라)/  keras 의 매개변수는 무엇을 요구하나? 딕셔너리 \n",
    "pipeline = Pipeline(estimators)    \n",
    "kfold =KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold) # 파이프라인 집어넣고 검증을 했다 .->즉 이말은 즉슨 standardscaler 도 해줬고, kerasregressor 도 해줬다 \n",
    "print('Standardized: %.2f ( %.2f) MSE' % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X,Y) # 그리고 나서 피팅을 해주었다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=pipeline.predict(X)  \n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.corrcoef(res[0], Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번엔 다른 모델 \n",
    "def larger_model():  # 좀 더 deep 해졌다. \n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer='normal',activation='relu' ))\n",
    "    model.add(Dense(6,kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "estimator = KerasRegressor(build_fn = larger_model, nb_epoch=100, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에 baseline 했던 것 처럼 한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 또다른 model 3 을 만든다 ( wider_model ) \n",
    "def wider_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1,kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimzer='adam')\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X,Y)\n",
    "res = pipeline.predict(X)\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이것도 평가한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.corrcoef(res,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이렇게 만들면서 모델중 어떤 것이 좋은지 판단하는 것은 사람의 일이다. ( 상관계수 높은 것을 찾아낸다)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과적합 방지 : dropout, regularizer, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숙제\n",
    "# - iris.csv   로딩한 다음 분류망을 구성하시오 ( house 예측한 것 처럼 망을 구성해본다 ) \n",
    "# -  parameter tuning 을 구현하시오 ( pipeline 사용도 함께 )   # 일요일저녁까지 제출"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
