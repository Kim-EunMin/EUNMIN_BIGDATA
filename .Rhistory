##단계 6) word cloud 작성
noun5
wordcount <- sort(table(noun5))
wordcloud(names(wordcount),freq=wordcount,scale=c(6,0.7),random.order=F, colors=pal,min.freq = 3)
wordcount <- sort(table(noun5))
wordcount
#문제3) 오바마 대통령의 데통령 당선 연설문에 대해 워드클라우드를 작성하시오
#       Tip. 예제소스 파일은 ‘ex_10-5.txt’이다.
##
library(KoNLP)
useSejongDic()
setwd("D:/EUNMIN_BIGDATA")
setwd("D:/EUNMIN_BIGDATA")
word_data <- readLines("ex_10-5.txt",encoding="UTF-8")
word_data2 <- sapply(word_data,extractNoun,USE.NAMES = F)
word_data2
undata
undata <- unlist( word_data2 )
undata
word_table <- table(undata)
word_table
undata2
undata2 <- undata[nchar(undata)>=2]
undata2
word_table2 <- table(undata2)
word_table2
sort(word_table2,decreasing=T)
sort(word_table2,decreasing=F)
undata2 <- gsub("당신들",undata2)
undata2
undata2 <- gsub("때문", "",undata2)
undata2 <- gsub("들이", "",undata2)
undata2 <- gsub("이것","",undata2)
undata2 <- gsub("당신들","",undata2)
table(undata2)
sort(table(undata2),decreasing = T)
sort(table(undata2),decreasing = F)
buildDictionary(user_dic =
data.frame("미국","ncn",
replace_usr_dic = T))
buildDictionary(user_dic =
data.frame("미국","ncn",
replace_usr_dic = T))
get_dictionary('user_dic')
buildDictionary(user_dic =
data.frame("미국","ncn",
replace_usr_dic = T))
get_dictionary('user_dic')
buildDictionary(user_dic =
data.frame("미국","ncn",
replace_usr_dic = T))
get_dictionary('user_dic')
undata2 <- gsub("당신들","",undata2)
buildDictionary(user_dic = data.frame("미국","ncn",
replace_usr_dic = T))
get_dictionary('user_dic')
buildDictionary(user_dic = data.frame("미국","ncn"),
replace_usr_dic = T)
get_dictionary('user_dic')
sort(table(undata2),decreasing = F)
word_table2 <- sort(table(undata2),decreasing = F)
library(wordcloud2)
wordcloud2(word_table2)
Sys.setenv( JAVA_HOME = 'C:/Program Files/Java/jre1.8.0_231' )
library( wordcloud )
library( KoNLP )
library( RColorBrewer )
setwd("d:./")
# ex_10-1.txt
text <- readLines( "ex_10-1.txt", encoding ="UTF-8" )
text
#1-1) ex_10-1 분석
##단계 1. 사전 설정
library(KoNLP)
useSejongDic()
##단계 2. 텍스트 데이터 가져오기
setwd("D:/EUNMIN_BIGDATA")
text1 <- readLines("ex_10-1.txt",encoding="UTF-8")
text1
##단계 3. 명사추출
text1_1 <- sapply(text1,extractNoun,USE.NAMES=F)
text1_1
source('D:/EUNMIN_BIGDATA/day12_answer.R', encoding = 'UTF-8', echo=TRUE)
buildDictionary( ext_dic = "woorimalsam" )
noun <- sapply( text, extractNoun, USE.NAMES = F )
noun
noun2 <- unlist(noun)
# 무의미한 단어 제거
noun2 <- noun2[ nchar(noun2) > 1 ]
noun2 <- gsub( "들이", "", noun2 )
noun2 <- gsub( "10", "", noun2 )
noun2 <- gsub( "이명", "", noun2 )
wordcount <- table( noun2 )
wordcloud2( wordcount,
size = 1.2, color = rep_len( c( 'red', 'steelblue'),
nrow( wordcount ) ),
shape = 'diamond' )
# ex_10-2.txt
useSejongDic()
text <- readLines( "ex_10-2.txt", encoding ="UTF-8" )
Sys.setenv( JAVA_HOME = 'C:/Program Files/Java/jre1.8.0_231' )
library( wordcloud )
library( KoNLP )
library( RColorBrewer )
setwd("d:./")
# ex_10-1.txt
text <- readLines( "ex_10-1.txt", encoding ="UTF-8" )
text
buildDictionary( ext_dic = "woorimalsam" )
noun <- sapply( text, extractNoun, USE.NAMES = F )
noun
noun2 <- unlist(noun)
# 무의미한 단어 제거
noun2 <- noun2[ nchar(noun2) > 1 ]
noun2 <- gsub( "들이", "", noun2 )
noun2 <- gsub( "10", "", noun2 )
noun2 <- gsub( "이명", "", noun2 )
wordcount <- table( noun2 )
wordcloud2( wordcount,
size = 1.2, color = rep_len( c( 'red', 'steelblue'),
nrow( wordcount ) ),
shape = 'diamond' )
library(wordcloud2)
wordcount <- table( noun2 )
wordcloud2( wordcount,
size = 1.2, color = rep_len( c( 'red', 'steelblue'),
nrow( wordcount ) ),
shape = 'diamond' )
library(KoNLP)
useSejongDic()
##단계 2. 텍스트 데이터 가져오기
setwd("D:/EUNMIN_BIGDATA")
text1 <- readLines("ex_10-1.txt",encoding="UTF-8")
text1
##단계 3. 명사추출
text1_1 <- sapply(text1,extractNoun,USE.NAMES=F)
text1_1
##단계 4. 백터로 변환
undata1 <- unlist(text1_1)
undata1
##단계 5. 사용빈도 확인
word_table1 <- table(undata1)
word_table1
##단계 6. 필터링 : 2글자 이상 단어만 선별, 불필요한 단어 삭제
undata1_1 <- undata1[nchar(undata1)>=2]
undata1_1 <- gsub("그것","",undata1_1)
undata1_1 <- gsub("여러분","",undata1_1)
undata1_1 <- gsub("그동안","",undata1_1)
undata1_1 <- gsub("민국","",undata1_1)
undata1_1 <- gsub("때문","",undata1_1)
word_table1_1 <- table(undata1_1)
word_table1_1
##단계 7. 데이터 정렬
sort(word_table1_1,decreasing=T)
library(wordcloud)
pal <- brewer.pal(8,"Dark2")
wordcloud(names(word_table1_1),freq=word_table1_1,scale=c(6,0.7),random.order=F, colors=pal,min.freq = 5)
##단계 7. 데이터 정렬
sort(word_table1_1,decreasing=T)
library(wordcloud)
pal <- brewer.pal(8,"Dark2")
wordcloud(names(word_table1_1),freq=word_table1_1,scale=c(6,0.7),random.order=F, colors=pal,min.freq = 5)
str( state.x77 )
head( state.x77 )
state.x77_df <- data.frame( state.x77 )
str( state.x77_df )
head( state.x77_df )
state.x77_model <- lm( Murder~Illiteracy , data = state.x77_df )
plot( Murder~Illiteracy , data = state.x77_df )
abline( state.x77_model )
coef( state.x77_model )
summary( state.x77_model )
state.x77 <- as.data.frame(state.x77)
plot(Murder~Illiteracy,data=state.x77)
model <- lm(Murder~Illiteracy,data=state.x77)
model
abline(model)
coef(model)
Murder = 4.257457 * state.x77_df$Illiteracy + 2.396776
Murder
fitted( state.x77_model )
residuals( state.x77_model )
# 회귀식
Murder = 4.257457 * state.x77_df$Illiteracy + 2.396776
Murder
fitted( state.x77_model )
residuals( state.x77_model )
# 범죄율 예측
Illiteracy_df <- data.frame( Illiteracy = c( 0.5, 1.0, 1.5 ) )
Murder_pred <- predict( state.x77_model, Illiteracy_df )
Murder_pred
new_Illiteracy <- data.frame(Illiteracy=c(0.5,1.0,1.5))
predict(model,new_Illiteracy)
plot(Volume~Girth,data=trees)
model <- lm(Volume~Girth,data=trees)
abline(model)
new_Girth <- data.frame(Girth=c(8.5,9.0,9.5))
predict(model,new_Girth)
plot(pressure~temperature,data=pressure)
model <- lm(pressure~temperature,data=pressure)
abline(model)
new_temperature <- data.frame(temperature=c(65,95,155))
predict(model,new_temperature)
library(car)
str(Prestige)
head(Prestige)
newdata <- Prestige[,c(1:4)]
head(newdata)
plot(newdata,pch=16,col='blue', main="Matrix Scatterplot")
model <- lm(income~education+prestige+women, data=newdata)  #그 다음 모델 만들어봄
model
coef(model)
fitted(model)
head(newdata)
residuals(model)
12351-11231.9859
deviance(model)
deviance(model)/ length(newdata$education)
summary(model)
#List
ds<-c(90,85,70,84)
my.info <- list(name="Hong", age=30,status=TRUE,
score=ds)
#List
ds<-c(90,85,70,84)
my.info <- list(name="Hong", age=30,status=TRUE,
score=ds)
my.info
my.info[1]                      #name,age,status,score 이 key다.(사진)
my.info[3]                      #name,age,status,score 이 key다.(사진)
my.info[[1]]          #실제 필요한건 value 일텐데, 대괄호 2개쓰는거 잊지말자
my.info[[2]]          #실제 필요한건 value 일텐데, 대괄호 2개쓰는거 잊지말자
my.info[[5]]          #실제 필요한건 value 일텐데, 대괄호 2개쓰는거 잊지말자
my.info[[3]]          #실제 필요한건 value 일텐데, 대괄호 2개쓰는거 잊지말자
my.info[[4]]          #실제 필요한건 value 일텐데, 대괄호 2개쓰는거 잊지말자
class(my.info[[1]])          #실제 필요한건 value 일텐데, 대괄호 2개쓰는거 잊지말자
class(my.info[[4]])          #실제 필요한건 value 일텐데, 대괄호 2개쓰는거 잊지말자
my.info$name
my.info
my.info[[4]]
my.info[[4]][1]        #4번째요소 즉 c(90,85,70,84) 벡터를 말하는 것 인데 거기서 첫번째니까 90
class(my.info[1])                      #name,age,status,score 이 key다.(사진)
summary(model)
summary(model2)
newdata2 <- Prestige[,c(1:5)]
model2 <- lm(income~., data=newdata2)
summary(model2)
summary(model)
summary(model2)
library(MASS )
model3 <- stepAIC (model2)
model3
summary(model3) #즉 모델 3은 women 과 prestige 를 가지고 만든 것(영향력 높은 것들만 가지고 만듬)
summary(model2)
summary(model2)
iris.new <- iris
iris.new$Species <- as.integer(iris.new$Species)
head(iris.new)
iris_model <- glm(Species~.,data=iris.new)
iris_model
coef(iris_model)
summary(iris_model)
#모델 만들었으니 이제 예측을 해보자.
unknwon <- data.frame(rbind(c(5.1,3.5,1.4,0.2)))
unknwon
unknwon2 <- data.frame(c(1,2,3,3))
unknwon2
names(unknwon) <- names(iris)[1:4]
unknwon
pred <- predict(iris_model,unknwon)
pred
#0.9174506 으로 나오는데 이게 어떤 품종인지 알 수 없다.
# 그래서 이때 one-hot encoding 을 한다.(1,2,3 으로 나눠야 보기 편하다)
# one-hot encoding 이 필요한 이유는? y 값이 범주형이기 때문이다.
round(pred,0)
#0.9174506 으로 나오는데 이게 어떤 품종인지 알 수 없다.
# 그래서 이때 one-hot encoding 을 한다.(1,2,3 으로 나눠야 보기 편하다)
# one-hot encoding 이 필요한 이유는? y 값이 범주형이기 때문이다.
round(pred,2)  #반올림
#0.9174506 으로 나오는데 이게 어떤 품종인지 알 수 없다.
# 그래서 이때 one-hot encoding 을 한다.(1,2,3 으로 나눠야 보기 편하다)
# one-hot encoding 이 필요한 이유는? y 값이 범주형이기 때문이다.
round(pred,0)  #반올림
#0.9174506 으로 나오는데 이게 어떤 품종인지 알 수 없다.
# 그래서 이때 one-hot encoding 을 한다.(1,2,3 으로 나눠야 보기 편하다)
# one-hot encoding 이 필요한 이유는? y 값이 범주형이기 때문이다.
pred <- round(pred,0)  #반올림
#즉 c(5.1,3.5,1.4,0.2)의 값을 값는 저것은 1번 품종이다.
pred
#즉 c(5.1,3.5,1.4,0.2)의 값을 값는 저것은 1번 품종이다.
levels(iris$Species)[pred]
test <- iris[,1:4]
pred <- predict(iris_model,test)
pred <- round(pred,0)
pred==answer
answer <- as.integer(iris$Species)
pred==answer
acc <- mean(pred==answer)
acc
test <- iris[,1:4]
test
iris_model
pred <- predict(iris_model,test)
pred <- round(pred,0)
answer <- as.integer(iris$Species)
pred==answer
#김은민
#2019-12-17 (HW)
#
#문제1) trees 데이터셋에 대해 다음의 문제를 해결하는 R 코드를 작성하시오.
# (1) 나무 둘레(Girth)와 나무의 키(Height)로 나무의 볼륨을 예측하는 다중선형 회귀모델을 만드시오.
newdata <- trees[,c("Girth","Height")]
head(newdata)
head(trees)
#김은민
#2019-12-17 (HW)
#
#문제1) trees 데이터셋에 대해 다음의 문제를 해결하는 R 코드를 작성하시오.
# (1) 나무 둘레(Girth)와 나무의 키(Height)로 나무의 볼륨을 예측하는 다중선형 회귀모델을 만드시오.
head(trees)
plot(trees,pch=16,col="red")
model1 <- lm(Volume~Girth+Height,data=trees)
model1
coef(model1)
Volume=(-57.9877)+(4.7082 * trees$Girth ) + (0.3393 * trees$Height)
fitted(model1)
#(2) 다중선형 회귀모델을 이용하여 trees 데이터셋의 나무 둘레(Girth)와 나무의 키(Height)로
#     나무의 볼륨을 예측하시오.
str(trees)
#사실 이걸 안해도됨
#plot(df$speed,predict(model,df),col="red",cex=2,pch=20)  #이걸 대체하는 함수가 predict 이다
#abline(model)
#predict(만들어둔 모델명, 맞는지확인할 새로운 x 값들 집합) - > 예측맞는지 확인
head(cars)
Volume
#(2) 다중선형 회귀모델을 이용하여 trees 데이터셋의 나무 둘레(Girth)와 나무의 키(Height)로
#     나무의 볼륨을 예측하시오.
fitted(model1)
Volume=(-57.9876589)+(4.7081605 * trees$Girth ) + (0.3392512 * trees$Height)
fitted(model1)
#fitted(model1)
Volume               #?? 둘 중 어떤건지
# (3) (2)에서 예측한 볼륨과 실제 trees 데이터셋의 볼륨(Volume)이 얼마나 차이가나는지 보이시오.
#       (예측값, 실제값, 예측값-실제값을 나타낸다.)
residuals(model)
# (3) (2)에서 예측한 볼륨과 실제 trees 데이터셋의 볼륨(Volume)이 얼마나 차이가나는지 보이시오.
#       (예측값, 실제값, 예측값-실제값을 나타낸다.)
residuals(model1)
compare <- data.frame(Volume,trees$Volume,Volume-trees$Volume)
colnames(compare) <- c("예측값","실제값","예측값-실제값")
compare
head(compare,3)
head(residuals(model1),3)
#문제2) mtcars 데이터셋에서 다른 변수들을 이용하여 연비(mpg)를 예측하는 다중 회귀모델을 만드시오.
# (1) 전체 변수를 이용하여 연비(mpg)를 예측하는 회귀모델을 만들고 회귀식을 나타내시오.
head(mtcars)
plot(mtcars,pch=16,col="blue")
model2 <- lm(mpg~.,data=mtcars)
model2
coef(model2)
MPG=(12.30337)- (0.11144  * mtcars$cyl ) + (0.01334  * mtcars$disp ) - (0.02148 * mtcars$hp ) +
(0.78711  * mtcars$drat ) - (3.71530 * mtcars$wt) + (0.82104  * mtcars$qsec ) +
(0.31776  * mtcars$vs )+ (2.52023  * mtcars$am ) + (0.65541  * mtcars$gear ) -
(0.19942 * mtcars$carb)
new_model2 <- stepAIC(model2)
new_model2
summary(new_model2)
summary(model2)
summary(new_model2)
new_model2
new_model2
MPG=(9.618)- (3.917 * mtcars$wt)+ (1.226  * mtcars$qsec )+(2.936  * mtcars$am )
library(MASS)
new_model2 <- stepAIC(model2)
new_model2
MPG=(9.618)- (3.917 * mtcars$wt)+ (1.226  * mtcars$qsec )+(2.936  * mtcars$am )
MPG
mtcars$mpg
MPG=(12.30337)- (0.11144  * mtcars$cyl ) + (0.01334  * mtcars$disp ) - (0.02148 * mtcars$hp ) +
(0.78711  * mtcars$drat ) - (3.71530 * mtcars$wt) + (0.82104  * mtcars$qsec ) +
(0.31776  * mtcars$vs )+ (2.52023  * mtcars$am ) + (0.65541  * mtcars$gear ) -
(0.19942 * mtcars$carb)
# (2) 연비(mpg)를 예측하는 데 도움이 되는 변수들만 사용하여 예측하는 회귀모델을만들고 회귀식을 나타내시오.
library(MASS)
new_model2 <- stepAIC(model2)
new_model2
MPG2=(9.618)- (3.917 * mtcars$wt)+ (1.226  * mtcars$qsec )+(2.936  * mtcars$am )
MPG2
MPG
# (3) (1), (2)에서 만든 예측모델의 설명력(Adjusted R-squared)을 비교하시오.
summary(model2)
summary(new_model2)
# (3) (1), (2)에서 만든 예측모델의 설명력(Adjusted R-squared)을 비교하시오.
summary(model2)
summary(new_model2)
mydata <- read.csv( "https://stats.idre.ucla.edu/stat/data/binary.csv" )
head(mydata)
#(1) gre, gpa, rank를 이용해 합격 여부(admit)를 예측하는 로지스틱 모델을 만드시오(0: 불합격, 1:합격).
mydata$admit
#(1) gre, gpa, rank를 이용해 합격 여부(admit)를 예측하는 로지스틱 모델을 만드시오(0: 불합격, 1:합격).
model3 <- glm(admit~.,data=mydata)
model3
coef(model3)
summary(model3)
coef(model3)
iris_model <- glm(Species~.,data=iris.new)  #로지스틱 회귀분석에 쓰는 함수 (lm 이 아니다)
iris_model    #종속변수(y)가 품종이다. x는 나머지 전부 (나중에 예측한 y 값은 범주형data이다)
#(1) gre, gpa, rank를 이용해 합격 여부(admit)를 예측하는 로지스틱 모델을 만드시오(0: 불합격, 1:합격).
model3 <- glm(admit~.,data=mydata)
model3
coef(model3)
summary(model3)
coef(model3)
head(mydata)
ADMIT =-(0.1824126752) + (0.0004424258*mydata$gre) + (0.1510402328*mydata$gpa) - (0.1095019242*mydata$rank)
ADMIT
ADMIT
unknwon <- mydata[,c(2:4)]
unknwon
head(unknwon)
pred <- predict(model3,unknwon)
pred
haed(pred,3)
head(ADMIT,3)
head(pred,3)
head(ADMIT,3)
compare <- data.frame(mydata$admit,pred)
head(compare,10)
pred <- round(pred,0)
head(pred,10)
compare2 <- data.frame(mydata$admit,pred)
compare2
head(compare2,10)
head(pred,10)
unknwon <- mydata[,c(2:4)]
pred <- predict(model3,unknwon)
pred
head(pred,10)
pred <- round(pred,0)
head(pred,10)
unknwon <- mydata[,c(2:4)]
pred <- predict(model3,unknwon)
compare <- data.frame(mydata$admit,pred)
pred <- round(pred,0)
compare2 <- data.frame(mydata$admit,pred)
head(compare2,10)
colnames(compare2) <- c("실제값","예측값")
head(compare2,10)
#(3) 만들어진 모델의 예측 정확도를 나타내시오.
pred
real_admit <- mydata$admit
pred==real_admit
acc <- mean(pred==real_admit)
acc
head(compare2,10)
#Factor 형 (범주형)
bt <- c('A','B','B','O','AB','A')    #bt는 문자형벡터, bt.new는 팩터형 벡터
#Factor 형 (범주형)
bt <- c('A','B','B','O','AB','A')    #bt는 문자형벡터, bt.new는 팩터형 벡터
bt.new <- factor(bt)
#Factor 형 (범주형)
bt <- c('A','B','B','O','AB','A')    #bt는 문자형벡터, bt.new는 팩터형 벡터
bt.new <- factor(bt)
bt
bt.new   #factor형일땐 levels 가 나옴(levels 는 가질 수 있는 값의 범위다,즉 앞으로 bt.new 에는 4종류외에 다른 값을 넣으면 안된다)
bt
bt[5]
bt.new[5]
levels(bt.new)
as.integer(bt.new)      #44,45page (is로 시작하면 결과값은 논리형, as 로 시작하면 변환함수)
#2019/11/28
#
bt.new
as.integer(bt.new)      #44,45page (is로 시작하면 결과값은 논리형, as 로 시작하면 변환함수)
#2019/11/28
bt.new[7]<-'B'
bt.new[8]<-'C'     #C는 levels 에 포함되는 값이 아니므로 에러가 뜬다.그러나 8번째가 요소가 생긴다. (NA로)
bt.new
library(tidyverse)
library(car)
str(Prestige)
head(Prestige) #독립변수 : education, women, prestige / 종속변수 : income (income을 예측할 것)
newdata <- Prestige[,c(1:4)]
head(newdata)
plot(newdata,pch=16,col='blue', main="Matrix Scatterplot")  #일단 4변수간의 연관성 분석해봄(상관분석)
model <- lm(income~education+prestige+women, data=newdata)  #그 다음 모델 만들어봄
model
coef(model)
income= (-253.8497) + (177.1990* newdata$education) +
(141.4354* newdata$prestige) -
(50.8957 * newdata$women)
income
head(income,5)
fitted(model)
head(income,5)
head(fitted(model),5)
residuals(model)
head(residuals(model),5)
head(newdata$income-income)
deviance(model)  #잔차
summary(model)
#이번엔 모든 변수 넣어보자
newdata2 <- Prestige[,c(1:5)]
head(newdata2)
head(Prestige)
#이번엔 모든 변수 넣어보자
newdata2 <- Prestige[,c(1:5)]
model2 <- lm(income~., data=newdata2)
summary(model2)
library(MASS)
model3 <- stepAIC (model2) #stepAIC 함수자체가 영향력 없는 변수 제거해봄(education 과 census 가 제거됨)
summary(model3)
