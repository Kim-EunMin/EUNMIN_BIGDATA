{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can  파일 길이 :  507\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-63e61fe5d597>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RGB\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np   # >> 데이터 처리에 사용\n",
    "from numpy import array\n",
    "caltech_dir = \"C:/Users/ICT01_09/Documents/CNN/\"\n",
    "categories = array(os.listdir(caltech_dir)) # 각 카테고리 폴더에서 불러오기\n",
    "nb_classes = len(categories)\n",
    "\n",
    "\n",
    "\n",
    "image_w = 100\n",
    "image_h = 100\n",
    "\n",
    "pixels = image_h * image_w * 3\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, cat in enumerate(categories):\n",
    "    \n",
    "    #one-hot 돌리기.\n",
    "    label = [0 for i in range(nb_classes)]\n",
    "    label[idx] = 1\n",
    "\n",
    "    image_dir = caltech_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir+\"/*.jpg\")\n",
    "    print(cat, \" 파일 길이 : \", len(files))\n",
    "    for i, f in enumerate(files):\n",
    "        \n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "\n",
    "        X.append(data)\n",
    "        y.append(label)\n",
    "        print(X)\n",
    "        if i % 700 == 0:\n",
    "            print(cat, \" : \", f)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "#1 0 0 0 이면 airplanes\n",
    "#0 1 0 0 이면 buddha 이런식\n",
    "\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "#xy = (X_train, X_test, y_train, y_test)\n",
    "#np.save(\"./multi_image_data.npy\", xy)\n",
    "\n",
    "#print(\"ok\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(caltech_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beagle_dir=os.path.join(caltech_dir,'beagle')\n",
    "os.listdir(beagle_dir)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name='beagle 199.jpg'\n",
    "img_path = os.path.join(beagle_dir, img_name)\n",
    "\n",
    "\n",
    "from keras.preprocessing import image\n",
    "img = image.load_img(img_path, target_size=(100, 100))\n",
    "img_tensor = image.img_to_array(img)\n",
    "\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각각 디렉토리 지정해주고 파일 불러와서 이미지 3d 텐서 => 4d 텐서 확인\n",
    "\n",
    "# 사용자 정의함수를 통해 2801개를 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img_path, target_size=100):\n",
    "    from keras.preprocessing import image\n",
    "    \n",
    "    img = image.load_img(img_path, target_size=(target_size, target_size))\n",
    "    img_tensor = image.img_to_array(img)\n",
    "    \n",
    "    # expand a dimension\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "    \n",
    "    #scaling into [0,1]\n",
    "    img_tensor /=255.\n",
    "    \n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 위에서 이미지 4000장 불러오면서 이미지벡터화 시킴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy\n",
    "import matplotlib\n",
    "import pandas\n",
    "import sklearn\n",
    "#import pydot\n",
    "import h5py\n",
    "\n",
    "import tensorflow\n",
    "import keras\n",
    "\n",
    "print('scipy ' + scipy.__version__)\n",
    "print('numpy ' + numpy.__version__)\n",
    "print('matplotlib ' + matplotlib.__version__)\n",
    "print('pandas ' + pandas.__version__)\n",
    "print('sklearn ' + sklearn.__version__)\n",
    "print('h5py ' + h5py.__version__)\n",
    "print('tensorflow ' + tensorflow.__version__)\n",
    "print('keras ' + keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, numpy as np\n",
    "import tensorflow.keras as keras\n",
    "# module 'tensorflow' has no attribute 'get_default_graph'\n",
    "\n",
    "from keras.models import Model, Input #고쳐야만 아래 소스 구동\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "import tensorflow as tf\n",
    "'''\n",
    "config = tf.ConfigProto()  텐서플로우 2. 에서 구동안됨\n",
    "module 'tensorflow' has no attribute 'ConfigProto'\n",
    "'''\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# session = tf.Session(config=config) 자동세션기능\n",
    "X_train, X_test, y_train, y_test = np.load('./multi_image_data.npy',allow_pickle=True)\n",
    "print(X_train.shape)\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"corgi\", \"golden\" ,\"dach\" ,\"husky\" ,\"beagle\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "#일반화\n",
    "X_train = X_train.astype(float) / 255\n",
    "X_test = X_test.astype(float) / 255\n",
    "\n",
    "\n",
    "\n",
    "#세개의 모델에서 같은 형태의 데이터를 사용할 것, 모든 모델에서 사용할 단일 입력 레이어를 정의.\n",
    "input_shape =X_train[0,:,:,:].shape\n",
    "model_input = keras.Input(shape =input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with K.tf_ops.device('/device:GPU:0'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model_dir = './model'\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    model_path = model_dir + '/multi_img_classification.model'\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path , monitor='test_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='test_loss', patience=6)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vloss = history.history['val_accuracy']\n",
    "y_loss = history.history['accuracy']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_accuracy')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_oss')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 앙상블하기\n",
    "https://github.com/KerasKorea/KEKOxTutorial/blob/master/16_Ensembling%20ConvNets%20using%20Keras.md\n",
    "\n",
    "https://github.com/mmxmb/keras_ensemblng/blob/master/keras_ensembling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 모델에 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Activation, Average\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.callbacks import History\n",
    "from keras.engine import training\n",
    "from keras.optimizers import Adam\n",
    "from typing import Tuple, List\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "import tensorflow as tf\n",
    "'''\n",
    "config = tf.ConfigProto()  텐서플로우 2. 에서 구동안됨\n",
    "module 'tensorflow' has no attribute 'ConfigProto'\n",
    "'''\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# session = tf.Session(config=config) 자동세션기능\n",
    "X_train, X_test, y_train, y_test = np.load('./multi_image_data.npy',allow_pickle=True)\n",
    "print(X_train.shape)\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 작업 폴더 얻기 os.getcwd()\n",
    "'''\n",
    "CONV_POOL_CNN_WEIGHT_FILE = os.path.join(os.getcwd(), 'weights', 'conv_pool_cnn_pretrained_weights.hdf5')\n",
    "ALL_CNN_WEIGHT_FILE = os.path.join(os.getcwd(), 'weights', 'all_cnn_pretrained_weights.hdf5')\n",
    "NIN_CNN_WEIGHT_FILE = os.path.join(os.getcwd(), 'weights', 'nin_cnn_pretrained_weights.hdf5')\n",
    "''' 에러나서 현재 못씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"corgi\", \"golden\" ,\"dach\" ,\"husky\" ,\"beagle\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "#일반화\n",
    "X_train = X_train.astype(float) / 255\n",
    "X_test = X_test.astype(float) / 255\n",
    "\n",
    "print('X_train shape: {} | y_train shape: {}\\nX_test shape : {} | y_test shape : {}'.format(\n",
    "    X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n",
    "\n",
    "\n",
    "#세개의 모델에서 같은 형태의 데이터를 사용할 것, 모든 모델에서 사용할 단일 입력 레이어를 정의.\n",
    "input_shape =X_train[0,:,:,:].shape\n",
    "model_input = Input(shape =input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델보다 작아서 훈련이 훨씬 빠름. 최종 Conv2D\n",
    "def nin_cnn(model_input):\n",
    "    \n",
    "    #mlpconv block 1\n",
    "    x = Conv2D(32, (5, 5), activation='relu',padding='valid')(model_input)\n",
    "    x = Conv2D(32, (1, 1), activation='relu')(x)\n",
    "    x = Conv2D(32, (1, 1), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    #mlpconv block2\n",
    "    x = Conv2D(64, (3, 3), activation='relu',padding='valid')(x)\n",
    "    x = Conv2D(64, (1, 1), activation='relu')(x)\n",
    "    x = Conv2D(64, (1, 1), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    #mlpconv block3\n",
    "    x = Conv2D(128, (3, 3), activation='relu',padding='valid')(x)\n",
    "    x = Conv2D(32, (1, 1), activation='relu')(x)\n",
    "    x = Conv2D(10, (1, 1))(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Activation(activation='softmax')(x)\n",
    "    \n",
    "    model = Model(model_input, x, name='nin_cnn')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "nin_cnn_model = nin_cnn(model_input)\n",
    "nin_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 단순화를 위해 각 모델은 동일한 매개변수를 사용하여 compile 되고 train\n",
    "32 배치사이즈로 1 epoch 당 1250번의 stpe??) 20 epoch로 사용하면 3가지 모델중 어떤모델이라도\n",
    "국소 최소치(local minimum)를 얻는데는 충분할 것으로 보임. \n",
    "훈련데이터 세트에서 무작위로 validation_split = 20% 데이터 검증으로 사용'''\n",
    "\n",
    "def compile_and_train(model, num_epochs):\n",
    "    model.compile(loss = categorical_crossentropy, optimizer=Adam(), metrics=['acc'])\n",
    "    '''filepath = 'weights/' + model.name + '.{epoch:02d}-{loss:.2f}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, \n",
    "                                 save_weights_only=True, save_best_only=True, mode='auto', period=1)\n",
    "    tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=32)\n",
    "    '''\n",
    "    history= model.fit(x=X_train, y=y_train, batch_size=32, epochs=num_epochs, verbose=1,\n",
    "                       validation_split =0.2) #callbacks=[checkpoint, tensor_board]\n",
    "    \n",
    "    # 위에서 weight 에러나서 callback 실현못함.. 누가 알아내줭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compile_and_train(nin_cnn_model, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nin_cnn_weight_file\n",
    "except NameError:\n",
    "    nin_cnn_model.load_weights(NIN_CNN_WEIGHT_FILE)\n",
    "evaluate_error(nin_cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_error(model):\n",
    "    pred = model.predict(X_test, batch_size=32)\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    pred = np.expand_dims(pred, axis=1) # make same shape as y_test\n",
    "    error = np.sum(np.not_equal(pred, y_test))/ y_test.shape[0]\n",
    "    return error\n",
    "\n",
    "# 모델 평가, 테스트세트의 에러율 계산하기\n",
    "'''\n",
    "try:\n",
    "    conv_pool_cnn_weight_file\n",
    "except NameError:\n",
    "    conv_pool_cnn_model.load_weights(CONV_POOL_CNN_WEIGHT_FILE)   \n",
    "evaluate_error(conv_pool_cnn_model)'''\n",
    "\n",
    "evaluate_error(all_cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_pool_cnn_model = conv_pool_cnn(model_input)\n",
    "all_cnn_model = all_cnn(model_input)\n",
    "nin_cnn_model = nin_cnn(model_input)\n",
    "\n",
    "conv_pool_cnn_model.load_weights('weights/conv_pool_cnn.29-0.10.hdf5')\n",
    "all_cnn_model.load_weights('weights/all_cnn.30-0.08.hdf5')\n",
    "nin_cnn_model.load_weights('weights/nin_cnn.30-0.93.hdf5')\n",
    "\n",
    "models = [conv_pool_cnn_model, all_cnn_model, nin_cnn_model]\n",
    "\n",
    "\n",
    "'''conv_pool_cnn_model = conv_pool_cnn(model_input)\n",
    "all_cnn_model = all_cnn(model_input)\n",
    "nin_cnn_model = nin_cnn(model_input)\n",
    "\n",
    "\n",
    "try:\n",
    "    conv_pool_cnn_model.load_weights(conv_pool_cnn_weight_file)\n",
    "except NameError:\n",
    "    conv_pool_cnn_model.load_weights(CONV_POOL_CNN_WEIGHT_FILE)\n",
    "try:\n",
    "    all_cnn_model.load_weights(all_cnn_weight_file)\n",
    "except NameError:\n",
    "    all_cnn_model.load_weights(ALL_CNN_WEIGHT_FILE)\n",
    "try:\n",
    "    nin_cnn_model.load_weights(nin_cnn_weight_file)\n",
    "except NameError:\n",
    "    nin_cnn_model.load_weights(NIN_CNN_WEIGHT_FILE)\n",
    "\n",
    "\n",
    "models = [conv_pool_cnn_model, all_cnn_model, nin_cnn_model]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(models, model_input):\n",
    "    \n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    y = Average()(outputs)\n",
    "    \n",
    "    model = Model(model_input, y, name='ensemble')\n",
    "    \n",
    "    return model\n",
    "\n",
    "ensemble_model = ensemble(models, model_input)\n",
    "\n",
    "evaluate_error(ensemble_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
